{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffad2080-e184-4bf0-bb22-363dcf4895e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available:  NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import config\n",
    "\n",
    "#!pip install timm==0.4.12\n",
    "\n",
    "# replace with pip install library\n",
    "from etab.utils.callbacks import *\n",
    "from etab.baselines.models import *\n",
    "from etab.datasets import *\n",
    "import etab\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(\"GPU(s) available: \", torch.cuda.get_device_name())\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"No GPUs available\")\n",
    "\n",
    "\n",
    "\n",
    "cuda_device      = 0\n",
    "device           = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd188d1-8a43-48b7-bc7f-10746e05ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#echonet_dataset = echonet(view=\"A4CH\")\n",
    "#echonet_dataset.load_data()\n",
    "\n",
    "# Pip install\n",
    "# basic usage\n",
    "\n",
    "# Add demo for data and functionalities:\n",
    "# - show segmentations\n",
    "# - show video gify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944515e5-083f-4e73-8878-db6ebb51f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model,\n",
    "                   train_loader, \n",
    "                   valid_loader, \n",
    "                   task_code, \n",
    "                   callbacks,\n",
    "                   n_epoch):\n",
    "    \n",
    "    save_base_dir = 'checkpoints'\n",
    "    # Reload the pretrained network and freeze it except for its head.\n",
    "    \n",
    "    seg_tasks     = [\"0\", \"1\", \"2\"]\n",
    "    class_tasks   = [\"3\", \"4\", \"5\", \"6\"]\n",
    "    \n",
    "    if task_code[-1] in seg_tasks:\n",
    "        \n",
    "        epoch_metric = ['f1', torchmetrics.JaccardIndex(num_classes=2)]\n",
    "        optimizer    = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        epoch_metric = ['f1']\n",
    "        optimizer    = optim.SGD(model.fc.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "    # Saves everything into ./saves/cub200_resnet18_experiment\n",
    "    save_path     = os.path.join(save_base_dir, task_code + \"_\" + backbone_type + \"_target\")\n",
    "\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_        = Model(model, \n",
    "                          optimizer, \n",
    "                          loss_function,\n",
    "                          batch_metrics=['accuracy'], \n",
    "                          epoch_metrics=epoch_metric,\n",
    "                          device=device)\n",
    "    \n",
    "    model_.fit_generator(train_loader, \n",
    "                         valid_loader, \n",
    "                         epochs=n_epoch, \n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c2c9c8-8f98-492f-a003-c3789d27f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark task code\n",
    "\n",
    "source_task      = \"EA40\"\n",
    "target_task      = \"CA45\"\n",
    "backbone_type    = \"resnet50\"\n",
    "\n",
    "benchmark_task   = source_task + target_task\n",
    "\n",
    "cuda_device      = 0\n",
    "device           = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes      = 200\n",
    "batch_size       = 32\n",
    "learning_rate    = 0.01\n",
    "n_epoch          = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776f650a-1857-4af4-a8da-3516ccb982d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset, target_dataset        = prepare_benchmark_data(source_task=source_task, \n",
    "                                                               target_task=target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbca6c32-9135-4496-8b60-5c65a74b564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train, source_val, source_test = ETAB_train_test_split(source_dataset, \n",
    "                                                              train_frac=0.6, \n",
    "                                                              val_frac=0.5)\n",
    "\n",
    "target_train, target_val, target_test = ETAB_train_test_split(target_dataset, \n",
    "                                                              train_frac=0.6, \n",
    "                                                              val_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7072ae6b-e67a-4341-8634-c06015494638",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_loader  = DataLoader(source_train, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "source_valid_loader  = DataLoader(source_val, batch_size=batch_size, num_workers=8)\n",
    "source_test_loader   = DataLoader(source_test, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "target_train_loader  = DataLoader(target_train, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "target_valid_loader  = DataLoader(target_val, batch_size=batch_size, num_workers=8)\n",
    "target_test_loader   = DataLoader(target_test, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3641c71-8d17-4c5f-8525-95cfba759a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce37950b-521e-481f-92d4-f8d6d22161b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_backbone  = prepare_ETAB_model(backbone_type, \n",
    "                                      pretrained=True, \n",
    "                                      mode=\"segmentation\",\n",
    "                                      num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceae4d6c-30b1-4e78-80b0-32e5750565f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks        = init_callbacks(benchmark_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5425c3-963d-4929-9eef-cec1dd3e41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m20.95s \u001b[35mloss:\u001b[94m 0.236740\u001b[35m acc:\u001b[94m 91.088460\u001b[35m fscore_macro:\u001b[94m 0.749394\u001b[35m jaccard_index:\u001b[94m 0.641705\u001b[35m val_loss:\u001b[94m 0.136479\u001b[35m val_acc:\u001b[94m 95.028235\u001b[35m val_fscore_macro:\u001b[94m 0.872467\u001b[35m val_jaccard_index:\u001b[94m 0.787739\u001b[0m\n",
      "Epoch 1: val_loss improved from inf to 0.13648, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 2/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.41s \u001b[35mloss:\u001b[94m 0.110880\u001b[35m acc:\u001b[94m 95.918420\u001b[35m fscore_macro:\u001b[94m 0.895260\u001b[35m jaccard_index:\u001b[94m 0.820368\u001b[35m val_loss:\u001b[94m 0.104998\u001b[35m val_acc:\u001b[94m 96.069074\u001b[35m val_fscore_macro:\u001b[94m 0.897062\u001b[35m val_jaccard_index:\u001b[94m 0.823155\u001b[0m\n",
      "Epoch 2: val_loss improved from 0.13648 to 0.10500, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 3/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.84s \u001b[35mloss:\u001b[94m 0.090333\u001b[35m acc:\u001b[94m 96.536895\u001b[35m fscore_macro:\u001b[94m 0.911439\u001b[35m jaccard_index:\u001b[94m 0.844723\u001b[35m val_loss:\u001b[94m 0.094365\u001b[35m val_acc:\u001b[94m 96.323102\u001b[35m val_fscore_macro:\u001b[94m 0.906630\u001b[35m val_jaccard_index:\u001b[94m 0.837329\u001b[0m\n",
      "Epoch 3: val_loss improved from 0.10500 to 0.09437, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 4/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.47s \u001b[35mloss:\u001b[94m 0.080462\u001b[35m acc:\u001b[94m 96.854433\u001b[35m fscore_macro:\u001b[94m 0.919518\u001b[35m jaccard_index:\u001b[94m 0.857303\u001b[35m val_loss:\u001b[94m 0.090352\u001b[35m val_acc:\u001b[94m 96.535764\u001b[35m val_fscore_macro:\u001b[94m 0.909230\u001b[35m val_jaccard_index:\u001b[94m 0.841453\u001b[0m\n",
      "Epoch 4: val_loss improved from 0.09437 to 0.09035, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 5/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.49s \u001b[35mloss:\u001b[94m 0.074176\u001b[35m acc:\u001b[94m 97.057997\u001b[35m fscore_macro:\u001b[94m 0.924680\u001b[35m jaccard_index:\u001b[94m 0.865488\u001b[35m val_loss:\u001b[94m 0.086995\u001b[35m val_acc:\u001b[94m 96.648127\u001b[35m val_fscore_macro:\u001b[94m 0.911981\u001b[35m val_jaccard_index:\u001b[94m 0.845683\u001b[0m\n",
      "Epoch 5: val_loss improved from 0.09035 to 0.08700, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 6/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.73s \u001b[35mloss:\u001b[94m 0.069058\u001b[35m acc:\u001b[94m 97.232920\u001b[35m fscore_macro:\u001b[94m 0.929165\u001b[35m jaccard_index:\u001b[94m 0.872695\u001b[35m val_loss:\u001b[94m 0.084614\u001b[35m val_acc:\u001b[94m 96.697307\u001b[35m val_fscore_macro:\u001b[94m 0.914205\u001b[35m val_jaccard_index:\u001b[94m 0.849069\u001b[0m\n",
      "Epoch 6: val_loss improved from 0.08700 to 0.08461, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 7/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.48s \u001b[35mloss:\u001b[94m 0.065253\u001b[35m acc:\u001b[94m 97.369073\u001b[35m fscore_macro:\u001b[94m 0.932541\u001b[35m jaccard_index:\u001b[94m 0.878184\u001b[35m val_loss:\u001b[94m 0.082660\u001b[35m val_acc:\u001b[94m 96.762298\u001b[35m val_fscore_macro:\u001b[94m 0.916431\u001b[35m val_jaccard_index:\u001b[94m 0.852502\u001b[0m\n",
      "Epoch 7: val_loss improved from 0.08461 to 0.08266, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 8/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.32s \u001b[35mloss:\u001b[94m 0.061629\u001b[35m acc:\u001b[94m 97.504840\u001b[35m fscore_macro:\u001b[94m 0.935985\u001b[35m jaccard_index:\u001b[94m 0.883835\u001b[35m val_loss:\u001b[94m 0.082500\u001b[35m val_acc:\u001b[94m 96.765860\u001b[35m val_fscore_macro:\u001b[94m 0.916352\u001b[35m val_jaccard_index:\u001b[94m 0.852387\u001b[0m\n",
      "Epoch 8: val_loss improved from 0.08266 to 0.08250, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 9/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.71s \u001b[35mloss:\u001b[94m 0.059050\u001b[35m acc:\u001b[94m 97.595374\u001b[35m fscore_macro:\u001b[94m 0.938347\u001b[35m jaccard_index:\u001b[94m 0.887738\u001b[35m val_loss:\u001b[94m 0.083193\u001b[35m val_acc:\u001b[94m 96.783752\u001b[35m val_fscore_macro:\u001b[94m 0.916079\u001b[35m val_jaccard_index:\u001b[94m 0.851999\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m10/10 \u001b[35mTrain steps: \u001b[36m140 \u001b[35mVal steps: \u001b[36m47 \u001b[32m19.40s \u001b[35mloss:\u001b[94m 0.056408\u001b[35m acc:\u001b[94m 97.698120\u001b[35m fscore_macro:\u001b[94m 0.940878\u001b[35m jaccard_index:\u001b[94m 0.891957\u001b[35m val_loss:\u001b[94m 0.084407\u001b[35m val_acc:\u001b[94m 96.761572\u001b[35m val_fscore_macro:\u001b[94m 0.915478\u001b[35m val_jaccard_index:\u001b[94m 0.851066\u001b[0m\n",
      "Restoring data from checkpoints/EA40CA45/best_weight.ckpt\n"
     ]
    }
   ],
   "source": [
    "source_model     = train_baseline(source_backbone,\n",
    "                                  source_train_loader, \n",
    "                                  source_valid_loader, \n",
    "                                  task_code=source_task, \n",
    "                                  callbacks=callbacks,\n",
    "                                  n_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1967acfe-cd28-4d27-be06-6e42c72ef5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model     = attach_head(source_backbone, \n",
    "                               backbone_type=backbone_type, \n",
    "                               source_mode=\"segmentation\",\n",
    "                               target_mode=\"classification\", \n",
    "                               num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e201731d-a0d2-40ec-806f-693d08c92d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_weights(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d441efd8-73e2-4775-8e0c-252ded8607fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks        = init_callbacks(benchmark_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e682e1-53b7-4bf9-9515-916cedc0a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.81s \u001b[35mloss:\u001b[94m 0.751541\u001b[35m acc:\u001b[94m 51.851852\u001b[35m fscore_macro:\u001b[94m 0.504125\u001b[35m val_loss:\u001b[94m 0.782565\u001b[35m val_acc:\u001b[94m 36.666667\u001b[35m val_fscore_macro:\u001b[94m 0.268293\u001b[0m\n",
      "Epoch 1: val_loss improved from inf to 0.78256, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 2/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.79s \u001b[35mloss:\u001b[94m 0.805765\u001b[35m acc:\u001b[94m 49.259259\u001b[35m fscore_macro:\u001b[94m 0.486671\u001b[35m val_loss:\u001b[94m 0.742051\u001b[35m val_acc:\u001b[94m 63.333333\u001b[35m val_fscore_macro:\u001b[94m 0.387755\u001b[0m\n",
      "Epoch 2: val_loss improved from 0.78256 to 0.74205, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 3/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.80s \u001b[35mloss:\u001b[94m 0.747555\u001b[35m acc:\u001b[94m 56.666667\u001b[35m fscore_macro:\u001b[94m 0.534476\u001b[35m val_loss:\u001b[94m 0.647095\u001b[35m val_acc:\u001b[94m 63.333334\u001b[35m val_fscore_macro:\u001b[94m 0.414778\u001b[0m\n",
      "Epoch 3: val_loss improved from 0.74205 to 0.64709, saving file to checkpoints/EA40CA45/best_weight.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 4/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.79s \u001b[35mloss:\u001b[94m 0.791618\u001b[35m acc:\u001b[94m 54.444444\u001b[35m fscore_macro:\u001b[94m 0.519941\u001b[35m val_loss:\u001b[94m 0.958941\u001b[35m val_acc:\u001b[94m 36.666667\u001b[35m val_fscore_macro:\u001b[94m 0.268293\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 5/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.78s \u001b[35mloss:\u001b[94m 0.641098\u001b[35m acc:\u001b[94m 61.851852\u001b[35m fscore_macro:\u001b[94m 0.595049\u001b[35m val_loss:\u001b[94m 0.762521\u001b[35m val_acc:\u001b[94m 43.333333\u001b[35m val_fscore_macro:\u001b[94m 0.424740\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 6/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.76s \u001b[35mloss:\u001b[94m 0.632187\u001b[35m acc:\u001b[94m 65.925926\u001b[35m fscore_macro:\u001b[94m 0.626466\u001b[35m val_loss:\u001b[94m 0.736225\u001b[35m val_acc:\u001b[94m 47.777779\u001b[35m val_fscore_macro:\u001b[94m 0.477197\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 7/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.81s \u001b[35mloss:\u001b[94m 0.688859\u001b[35m acc:\u001b[94m 58.148148\u001b[35m fscore_macro:\u001b[94m 0.568403\u001b[35m val_loss:\u001b[94m 0.672538\u001b[35m val_acc:\u001b[94m 63.333333\u001b[35m val_fscore_macro:\u001b[94m 0.478856\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 8/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.78s \u001b[35mloss:\u001b[94m 0.617177\u001b[35m acc:\u001b[94m 63.703704\u001b[35m fscore_macro:\u001b[94m 0.591667\u001b[35m val_loss:\u001b[94m 0.717245\u001b[35m val_acc:\u001b[94m 63.333333\u001b[35m val_fscore_macro:\u001b[94m 0.387755\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 9/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m1.77s \u001b[35mloss:\u001b[94m 0.634117\u001b[35m acc:\u001b[94m 64.444445\u001b[35m fscore_macro:\u001b[94m 0.613849\u001b[35m val_loss:\u001b[94m 0.733151\u001b[35m val_acc:\u001b[94m 48.888889\u001b[35m val_fscore_macro:\u001b[94m 0.487877\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m10/10 \u001b[35mTrain steps: \u001b[36m9 \u001b[35mVal steps: \u001b[36m3 \u001b[32m2.50s \u001b[35mloss:\u001b[94m 0.629778\u001b[35m acc:\u001b[94m 62.962963\u001b[35m fscore_macro:\u001b[94m 0.604546\u001b[35m val_loss:\u001b[94m 0.738850\u001b[35m val_acc:\u001b[94m 50.000000\u001b[35m val_fscore_macro:\u001b[94m 0.498452\u001b[0m\n",
      "Restoring data from checkpoints/EA40CA45/best_weight.ckpt\n"
     ]
    }
   ],
   "source": [
    "target_model     = train_baseline(target_model, \n",
    "                                  target_train_loader,\n",
    "                                  target_valid_loader,\n",
    "                                  task_code=target_task, \n",
    "                                  callbacks=callbacks,\n",
    "                                  n_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "336a54db-b975-491e-955a-d30537c68544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a single training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e090945-889a-4c68-a068-b63804f2f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zip -r etab.zip etab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ba86e0-77a5-4522-adc7-a026de694b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def create_echo_clip(one_patient_echo, op_file_name):\n",
    "    \n",
    "    echo_frames = []\n",
    "\n",
    "    for u in range(len(one_patient_echo)):\n",
    "        \n",
    "        echo_image  = one_patient_echo[u]\n",
    "\n",
    "        echo_frames.append(PIL_transform(echo_frames.detach().numpy().astype(np.uint8)))\n",
    "    \n",
    "    echo_frames[0].save(op_file_name, \n",
    "                        save_all=True, \n",
    "                        append_images=echo_frames[1:], optimize=False, duration=50, loop=0)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93563b-ca81-4141-93c4-0b437b0c66a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731107c9-0119-4d32-97c0-0fa8edcfc73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import torchvision\n",
    "import etab.utils.echonet\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0231a74b-4a75-4109-8a21-7342be58a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmented_data(data_dir, n_train=None, concatenate=True, rgb=False, IMG_SIZE=224, n_frames=1):\n",
    "    \n",
    "    transform    = T.ToPILImage()\n",
    "    pil_2_tensor = T.ToTensor()\n",
    "    \n",
    "    dataset      = Echo(root=data_dir + \"/data/\", target_type=\"SmallTrace\")\n",
    "\n",
    "    n_train      = len(dataset) if n_train is None else n_train\n",
    "    \n",
    "    videos       = []\n",
    "    segments     = []\n",
    "\n",
    "    for _ in range(n_train):\n",
    "  \n",
    "        current_video, current_trace = dataset.__getitem__(_) \n",
    "\n",
    "        if rgb:\n",
    "            \n",
    "            current_video, current_trace = torch.einsum('cnhw->nhwc', torch.tensor(current_video)/255)[:n_frames, :, :, :], torch.tensor(gray2rgb(current_trace))[:n_frames, :, :, :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            current_video = torch.einsum('cnhw->nhwc', torch.tensor(current_video)/255)[:n_frames, :, :, :]                \n",
    "            current_trace = torch.tensor(current_trace) \n",
    "            \n",
    "            if current_video.shape[0]==1:\n",
    "                \n",
    "                current_video = current_video.squeeze(0)\n",
    "                current_video = pil_2_tensor(transform(torch.einsum('hwc->chw', current_video)).resize((IMG_SIZE, IMG_SIZE)))\n",
    "                current_trace = pil_2_tensor(transform(current_trace).resize((IMG_SIZE, IMG_SIZE))).squeeze(0)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # loop over frames\n",
    "                current_video = [pil_2_tensor(transform(torch.einsum('hwc->chw', current_video[kk, :, :, :])).resize((IMG_SIZE, IMG_SIZE))).unsqueeze(0) for kk in range(current_video.shape[0])]\n",
    "                current_video = torch.cat(current_video, dim=0)\n",
    "                current_trace = pil_2_tensor(transform(current_trace).resize((IMG_SIZE, IMG_SIZE))).squeeze(0)     \n",
    "            \n",
    "\n",
    "        videos.append(current_video.unsqueeze(0))\n",
    "        segments.append(current_trace.unsqueeze(0))  \n",
    "    \n",
    "    if concatenate:\n",
    "        \n",
    "        videos       = torch.cat(videos)\n",
    "        segments     = torch.cat(segments) \n",
    "        all_data_set = torch.cat([videos, segments], dim=1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        all_data_set = [(videos[k].squeeze(0), segments[k].squeeze(0).type(torch.LongTensor)) for k in range(len(videos))]\n",
    "    \n",
    "    return all_data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7e6f54-3ae7-4f09-9e11-253f4a28c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = load_segmented_data(data_dir=config.echonet_dir, \n",
    "                                       n_train=None, \n",
    "                                       concatenate=False, \n",
    "                                       rgb=False, \n",
    "                                       IMG_SIZE=224, n_frames=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e9a09-a508-450a-813a-ee0871bce14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "#plt.imshow(data_[0][0][0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5960f8-5131-4415-879b-2f490e290cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reate_echo_clip(one_patient_echo, op_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d828c2-1cd3-48b2-bb38-4cc3217b7f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125cf19-bb30-46ce-b4de-884a14a30539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
